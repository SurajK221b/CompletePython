{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cda5ee",
   "metadata": {},
   "source": [
    "## Tokenization Using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f49a0",
   "metadata": {},
   "source": [
    "### Tokenization refers to the process of splitting text into smaller units such as sentences or words. NLTK provides robust tokenizers based on the Punkt and Treebank algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a280d52",
   "metadata": {},
   "source": [
    "### Prerequisite: Install and Download Required Resources\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download required datasets\n",
    "nltk.download('punkt')         # For sentence and word tokenizers\n",
    "nltk.download('averaged_perceptron_tagger')  # Optional, for POS tagging post-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51b46ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, I am Suraj Khodade. I am Tech Software developer! Tech Enthu.\\nPlease do connect with me on LinkedIn. It's a great platform to network and learn.\\nI am a Data Science Enthusiast.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initiate One Corpus\n",
    "\n",
    "corpus = \"\"\"Hello, I am Suraj Khodade. I am Tech Software developer! Tech Enthu.\n",
    "Please do connect with me on LinkedIn. It's a great platform to network and learn.\n",
    "I am a Data Science Enthusiast.\"\"\"\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2932a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required datasets\n",
    "nltk.download('punkt')         # For sentence and word tokenizers\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42e9cba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, I am Suraj Khodade.', 'I am Tech Software developer!', 'Tech Enthu.', 'Please do connect with me on LinkedIn.', \"It's a great platform to network and learn.\", 'I am a Data Science Enthusiast.']\n",
      "['Hello, I am Suraj Khodade.', 'I am Tech Software developer!', 'Tech Enthu.', 'Please do connect with me on LinkedIn.', \"It's a great platform to network and learn.\", 'I am a Data Science Enthusiast.']\n",
      "['Hello, I am Suraj Khodade.', 'I am Tech Software developer!', 'Tech Enthu.', 'Please do connect with me on LinkedIn.', \"It's a great platform to network and learn.\", 'I am a Data Science Enthusiast.']\n"
     ]
    }
   ],
   "source": [
    "##  1. Sentence Tokenization\n",
    "## Objective: Split a paragraph into individual sentences.\n",
    "\n",
    "text = nltk.sent_tokenize(corpus)   \n",
    "print(text)\n",
    "\n",
    "text = nltk.sent_tokenize(corpus, language='english')  # Specify language if needed\n",
    "print(text)\n",
    "\n",
    "text_german = nltk.sent_tokenize(corpus, language='german')  # Example for German\n",
    "print(text_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c767bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', 'am', 'Suraj', 'Khodade', '.', 'I', 'am', 'Tech', 'Software', 'developer', '!', 'Tech', 'Enthu', '.', 'Please', 'do', 'connect', 'with', 'me', 'on', 'LinkedIn', '.', 'It', \"'s\", 'a', 'great', 'platform', 'to', 'network', 'and', 'learn', '.', 'I', 'am', 'a', 'Data', 'Science', 'Enthusiast', '.']\n",
      "['Hello', ',', 'I', 'am', 'Suraj', 'Khodade', '.', 'I', 'am', 'Tech', 'Software', 'developer', '!', 'Tech', 'Enthu', '.', 'Please', 'do', 'connect', 'with', 'me', 'on', 'LinkedIn', '.', 'It', \"'s\", 'a', 'great', 'platform', 'to', 'network', 'and', 'learn', '.', 'I', 'am', 'a', 'Data', 'Science', 'Enthusiast', '.']\n",
      "['Hello', ',', 'I', 'am', 'Suraj', 'Khodade', '.', 'I', 'am', 'Tech', 'Software', 'developer', '!', 'Tech', 'Enthu', '.', 'Please', 'do', 'connect', 'with', 'me', 'on', 'LinkedIn', '.', 'It', \"'s\", 'a', 'great', 'platform', 'to', 'network', 'and', 'learn', '.', 'I', 'am', 'a', 'Data', 'Science', 'Enthusiast', '.']\n"
     ]
    }
   ],
   "source": [
    "##  2. Word Tokenization\n",
    "## Objective: Tokenize a sentence into individual words and punctuation marks.\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = word_tokenize(corpus)\n",
    "print(word_tokens)\n",
    "word_tokens = word_tokenize(corpus, language='english')  # Specify language if needed\n",
    "print(word_tokens)\n",
    "word_tokens_german = word_tokenize(corpus, language='german')  # Example for German\n",
    "print(word_tokens_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d4fd5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions', '.']\n"
     ]
    }
   ],
   "source": [
    "## 3. Treebank Tokenizer (Advanced Word Tokenizer)\n",
    "## Provides more refined tokenization, closer to linguistic accuracy.\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(\"Don't hesitate to ask questions.\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f562eeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'\", 's', 'test', ':', 'email', '@', 'example', '.', 'com', '!']\n"
     ]
    }
   ],
   "source": [
    "## 4. WordPunct Tokenizer (Splits punctuation from words)\n",
    "## Useful when punctuation separation is important.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(\"Let's test: email@example.com!\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c642c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'costs', '$1.99', 'and', 'that', 'costs', '$2.99']\n"
     ]
    }
   ],
   "source": [
    "## 5. Regex-Based Tokenization\n",
    "## Custom tokenization using regular expressions.\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "text = \"This costs $1.99, and that costs $2.99.\"\n",
    "tokens = regexp_tokenize(text, pattern=r'\\$\\d+\\.\\d+|\\w+')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65633e5",
   "metadata": {},
   "source": [
    "#### Comparative Analysis of NLTK Tokenization Methods\n",
    "\n",
    "| **Aspect**               | **Sentence Tokenization** (`sent_tokenize`) | **Word Tokenization** (`word_tokenize`) | **Treebank Tokenization** (`TreebankWordTokenizer`) | **Regex-Based Tokenization** (`RegexpTokenizer`)     |\n",
    "| ------------------------ | ------------------------------------------- | --------------------------------------- | --------------------------------------------------- | ---------------------------------------------------- |\n",
    "| **Purpose**              | Split paragraph into individual sentences   | Split sentence into words               | Precise word splitting based on Penn Treebank rules | Tokenize based on custom patterns                    |\n",
    "| **Granularity**          | Sentence-level                              | Word-level                              | Word-level                                          | Pattern-level (e.g., words, symbols, entities)       |\n",
    "| **Handles Punctuation**  | Retains sentence-ending punctuation         | Yes (e.g., `.`, `!`, `?`)               | Yes, intelligently separates contractions           | As per regex pattern                                 |\n",
    "| **Customization**        | Minimal (language model-based)              | Minimal                                 | Minimal                                             | Fully customizable using regex                       |\n",
    "| **Multilingual Support** | Yes (via Punkt model)                       | Limited                                 | English-centric                                     | Yes (regex-agnostic)                                 |\n",
    "| **Syntax**               | `sent_tokenize(text)`                       | `word_tokenize(text)`                   | `TreebankWordTokenizer().tokenize(text)`            | `RegexpTokenizer(pattern).tokenize(text)`            |\n",
    "| **Tokenizer Class**      | `PunktSentenceTokenizer`                    | `PunktWordTokenizer` (used internally)  | `TreebankWordTokenizer`                             | `RegexpTokenizer`                                    |\n",
    "| **Example Input**        | `\"Dr. Smith went home. He slept.\"`          | `\"Let’s write Python!\"`                 | `\"Don't panic.\"`                                    | `\"Price: $12.50 or ₹99\"`                             |\n",
    "| **Example Output**       | `[\"Dr. Smith went home.\", \"He slept.\"]`     | `[\"Let\", \"’s\", \"write\", \"Python\", \"!\"]` | `[\"Do\", \"n't\", \"panic\", \".\"]`                       | `[\"Price\", \"$12.50\", \"or\", \"₹99\"]`                   |\n",
    "| **Use Cases**            | Document parsing, summarization             | Preprocessing, feature extraction       | Linguistic analysis, POS tagging                    | Financial extraction, NER preprocessing, log parsing |\n",
    "| **Performance**          | High (pretrained model)                     | High                                    | High                                                | Medium–High (regex complexity dependent)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8516fc1",
   "metadata": {},
   "source": [
    "#### Summary Recommendation\n",
    "\n",
    "| Scenario                           | Recommended Tokenizer                      |\n",
    "| ---------------------------------- | ------------------------------------------ |\n",
    "| Sentence splitting                 | `sent_tokenize()`                          |\n",
    "| General NLP preprocessing          | `word_tokenize()`                          |\n",
    "| Linguistic precision (POS, syntax) | `TreebankWordTokenizer()`                  |\n",
    "| Pattern-driven tasks (custom)      | `RegexpTokenizer()` or `regexp_tokenize()` |\n",
    "\n",
    "##### Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "##### Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(text)\n",
    "\n",
    "##### Treebank Tokenization\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "##### Regex Tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\$\\d+\\.\\d+|\\w+')\n",
    "custom_tokens = tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1695e28",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
