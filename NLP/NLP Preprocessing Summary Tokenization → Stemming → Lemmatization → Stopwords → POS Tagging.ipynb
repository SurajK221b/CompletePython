{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e258964",
   "metadata": {},
   "source": [
    "## 1. Tokenization – Sentence and Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f51cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing is amazing. It powers AI applications.\"\n",
    "print(\"Sentences:\", sent_tokenize(text))\n",
    "print(\"Words:\", word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ba1f5",
   "metadata": {},
   "source": [
    "## 2. Stemming – Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72769109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"connected\", \"connecting\", \"connection\"]\n",
    "print([stemmer.stem(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15048ac5",
   "metadata": {},
   "source": [
    "## 3. Lemmatization – With POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "sentence = \"The leaves are falling off the tree and flying around.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40198f8",
   "metadata": {},
   "source": [
    "## 4. Stopword Removal – English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc97558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"This is a demonstration of how to remove stopwords.\"\n",
    "filtered = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
    "print(filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c9285",
   "metadata": {},
   "source": [
    "## 5. Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5305b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "sentence = \"John is writing a book about artificial intelligence.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(pos_tag(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e33913",
   "metadata": {},
   "source": [
    "## 6. Extract Named Entities as Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf67aed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tree\n\u001b[32m      3\u001b[39m named_entities = []\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m subtree \u001b[38;5;129;01min\u001b[39;00m \u001b[43mentities\u001b[49m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subtree, Tree):\n\u001b[32m      6\u001b[39m         entity_name = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join([token \u001b[38;5;28;01mfor\u001b[39;00m token, pos \u001b[38;5;129;01min\u001b[39;00m subtree.leaves()])\n",
      "\u001b[31mNameError\u001b[39m: name 'entities' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tree import Tree\n",
    "\n",
    "named_entities = []\n",
    "for subtree in entities:\n",
    "    if isinstance(subtree, Tree):\n",
    "        entity_name = \" \".join([token for token, pos in subtree.leaves()])\n",
    "        entity_type = subtree.label()\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "\n",
    "print(named_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c06748",
   "metadata": {},
   "source": [
    "### Summary Table\n",
    "\n",
    "| Step                 | Function               | Method                            | Sample Output                    |\n",
    "| -------------------- | ---------------------- | --------------------------------- | -------------------------------- |\n",
    "| **Tokenization**     | Sentence/word split    | `sent_tokenize`, `word_tokenize`  | `[\"Hello\", \"world\"]`             |\n",
    "| **Stemming**         | Word root via rules    | `PorterStemmer().stem()`          | `\"connected\"` → `\"connect\"`      |\n",
    "| **Lemmatization**    | Base form via WordNet  | `WordNetLemmatizer().lemmatize()` | `\"running\"` → `\"run\"` (with POS) |\n",
    "| **Stopword Removal** | Remove low-value words | `word not in stopwords.words()`   | `\"This is NLP\"` → `\"['NLP']\"`    |\n",
    "| **POS Tagging**      | Grammatical label      | `pos_tag()`                       | `(\"running\", \"VBG\")`             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb47c0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
