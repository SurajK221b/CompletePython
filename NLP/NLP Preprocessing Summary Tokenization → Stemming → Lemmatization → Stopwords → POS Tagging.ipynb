{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "765c586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Suraj\n",
      "[nltk_data]     Khodade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Suraj\n",
      "[nltk_data]     Khodade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Suraj\n",
      "[nltk_data]     Khodade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Suraj Khodade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Suraj\n",
      "[nltk_data]     Khodade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e258964",
   "metadata": {},
   "source": [
    "## 1. Tokenization – Sentence and Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27f51cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Natural Language Processing is amazing.', 'It powers AI applications.']\n",
      "Words: ['Natural', 'Language', 'Processing', 'is', 'amazing', '.', 'It', 'powers', 'AI', 'applications', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing is amazing. It powers AI applications.\"\n",
    "print(\"Sentences:\", sent_tokenize(text))\n",
    "print(\"Words:\", word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ba1f5",
   "metadata": {},
   "source": [
    "## 2. Stemming – Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72769109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['connect', 'connect', 'connect']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"connected\", \"connecting\", \"connection\"]\n",
    "print([stemmer.stem(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15048ac5",
   "metadata": {},
   "source": [
    "## 3. Lemmatization – With POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d174b478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'leaf', 'be', 'fall', 'off', 'the', 'tree', 'and', 'fly', 'around', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "sentence = \"The leaves are falling off the tree and flying around.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40198f8",
   "metadata": {},
   "source": [
    "## 4. Stopword Removal – English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edc97558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demonstration', 'remove', 'stopwords', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"This is a demonstration of how to remove stopwords.\"\n",
    "filtered = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
    "print(filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c9285",
   "metadata": {},
   "source": [
    "## 5. Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f5305b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP'), ('is', 'VBZ'), ('writing', 'VBG'), ('a', 'DT'), ('book', 'NN'), ('about', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "sentence = \"John is writing a book about artificial intelligence.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(pos_tag(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e33913",
   "metadata": {},
   "source": [
    "## 6. Extract Named Entities as Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daf67aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tree import Tree\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "\n",
    "# Use the sentence variable already defined in the notebook\n",
    "entities = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "named_entities = []\n",
    "for subtree in entities:\n",
    "    if isinstance(subtree, Tree):\n",
    "        entity_name = \" \".join([token for token, pos in subtree.leaves()])\n",
    "        entity_type = subtree.label()\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "\n",
    "print(named_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c06748",
   "metadata": {},
   "source": [
    "### Summary Table\n",
    "\n",
    "| Step                 | Function               | Method                            | Sample Output                    |\n",
    "| -------------------- | ---------------------- | --------------------------------- | -------------------------------- |\n",
    "| **Tokenization**     | Sentence/word split    | `sent_tokenize`, `word_tokenize`  | `[\"Hello\", \"world\"]`             |\n",
    "| **Stemming**         | Word root via rules    | `PorterStemmer().stem()`          | `\"connected\"` → `\"connect\"`      |\n",
    "| **Lemmatization**    | Base form via WordNet  | `WordNetLemmatizer().lemmatize()` | `\"running\"` → `\"run\"` (with POS) |\n",
    "| **Stopword Removal** | Remove low-value words | `word not in stopwords.words()`   | `\"This is NLP\"` → `\"['NLP']\"`    |\n",
    "| **POS Tagging**      | Grammatical label      | `pos_tag()`                       | `(\"running\", \"VBG\")`             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb47c0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
