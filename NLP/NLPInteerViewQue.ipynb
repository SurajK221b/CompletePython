{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d19ec4",
   "metadata": {},
   "source": [
    "### Tokenization in NLP – Interview Theory Sheet\n",
    "\n",
    "#### 1. What is tokenization?\n",
    "##### Answer:\n",
    "Tokenization is the process of splitting raw text into smaller units called tokens, which can be words, subwords, characters, or sentences. It is the foundational step in most NLP pipelines.\n",
    "\n",
    "#### 2. Why is tokenization important in NLP?\n",
    "##### Answer:\n",
    "Tokenization transforms unstructured text into structured tokens, enabling subsequent tasks like POS tagging, parsing, sentiment analysis, and language modeling. Without tokenization, models cannot differentiate between linguistic units.\n",
    "\n",
    "#### 3. What are different types of tokenization?\n",
    "Type | Description | Example\n",
    "--- | --- | ---\n",
    "Word | Splits text into words | \"Let's go\" → [\"Let\", \"'s\", \"go\"]\n",
    "Sentence | Splits text into sentences | \"Hi. How are you?\" → [\"Hi.\", \"How are you?\"]\n",
    "Subword | Used in BPE, WordPiece | \"playing\" → [\"play\", \"##ing\"]\n",
    "Character | Splits into individual characters | \"cat\" → [\"c\", \"a\", \"t\"]\n",
    "Regex-based | Custom rules to tokenize specific patterns | Emails, hashtags, prices, etc.\n",
    "\n",
    "#### 4. Explain the difference between word_tokenize and TreebankWordTokenizer.\n",
    "Feature | word_tokenize() | TreebankWordTokenizer()\n",
    "--- | --- | ---\n",
    "Backend | Wrapper over Treebank + Punkt | Manual usage of Treebank rules\n",
    "Output Format | Words + Punctuation | Words + Punctuation\n",
    "Usage Simplicity | Easier, automatic | More customizable\n",
    "Preferred For | Quick NLP tasks | Linguistic parsing\n",
    "\n",
    "#### 5. What is the role of sent_tokenize()?\n",
    "##### Answer:\n",
    "sent_tokenize() breaks a paragraph or document into sentences using pretrained models (e.g., Punkt tokenizer). It handles abbreviations, punctuation, and sentence boundaries intelligently.\n",
    "\n",
    "#### 6. What is RegexpTokenizer and where is it used?\n",
    "##### Answer:\n",
    "RegexpTokenizer allows custom tokenization using regular expressions. It is ideal for domain-specific extraction such as:\n",
    "\n",
    "- Extracting monetary values\n",
    "- Tokenizing hashtags, URLs, and dates\n",
    "- Tokenizing programming code or log files\n",
    "\n",
    "#### 7. What are the challenges in tokenization?\n",
    "##### Answer:\n",
    "- Ambiguity: \"U.S.\" vs. sentence end\n",
    "- Contractions: \"Don't\" → [\"Do\", \"n't\"]\n",
    "- Multilingual complexities\n",
    "- Special symbols or emojis\n",
    "- Compound words in German, Chinese (no whitespace)\n",
    "\n",
    "#### 8. Compare NLTK vs spaCy vs HuggingFace tokenizers\n",
    "Feature | NLTK | spaCy | HuggingFace\n",
    "--- | --- | --- | ---\n",
    "Language Support | English, multilingual | Multilingual | Multilingual\n",
    "Customization | Medium | High | Very High (subword-level)\n",
    "Speed | Medium | Very Fast | Fast\n",
    "Deep Learning | Not built-in | Limited | Deep Learning optimized\n",
    "Use Case | Education, research | Production NLP | Transformers, BERT, GPT\n",
    "\n",
    "#### 9. What is subword tokenization? Where is it used?\n",
    "##### Answer:\n",
    "Subword tokenization breaks unknown or rare words into smaller known units (e.g., \"unhappiness\" → [\"un\", \"happi\", \"ness\"]). It's essential for models like BERT, GPT, and T5 to reduce vocabulary size while covering more words.\n",
    "\n",
    "#### 10. How do you handle tokenization for Indian languages (e.g., Hindi, Tamil)?\n",
    "##### Answer:\n",
    "Tokenization for Indian languages often involves:\n",
    "\n",
    "- Unicode normalization\n",
    "- Syllable or morpheme-based tokenizers\n",
    "- Tools like Indic NLP Library, spaCy + custom rules, or HuggingFace tokenizers trained on native corpora\n",
    "\n",
    "✅ Pro Interview Tips  \n",
    "Always mention trade-offs (e.g., speed vs accuracy)  \n",
    "Reference real-world cases (e.g., Twitter sentiment, resume parsing)  \n",
    "Be prepared to write regex for pattern-based tokenization  \n",
    "Understand how tokenization impacts vectorization and embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f837cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
